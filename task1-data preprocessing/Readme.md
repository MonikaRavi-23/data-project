# ğŸ§  Data Preprocessing Challenge 

This project demonstrates **data preprocessing** using **Apache Spark (PySpark)**.  
It handles missing values, duplicates, data type inconsistencies, normalization, and feature engineering.


## ğŸš€ Features
- ğŸ§© Handle missing values (numeric + categorical)
- ğŸ”¢ Fix inconsistent data types
- ğŸ§¹ Remove duplicates
- âš–ï¸ Normalize/Standardize numerical features
- ğŸ§  Create engineered features


## ğŸ§° Tools & Technologies
- **Apache Spark (PySpark)**
- **Python 3.8+**
- **Parquet** (for optimized output)


## ğŸ“¦ Setup Instructions

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/yourusername/data-preprocessing-challenge.git
cd data-preprocessing-challenge

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the Preprocessing Script
Make sure your raw data is inside the data/ folder.

cd src
python preprocess.py

data/processed_dataset.parquet

ğŸ“‚ Input/Output
Type	Format	Path
Input	CSV	data/raw_dataset.csv
Output	Parquet	data/processed_dataset.parquet

âš™ï¸ Example Operations
Task	Description
Missing values	Numeric â†’ Mean fill, Categorical â†’ â€œUnknownâ€
Data types	Convert string-numeric to DoubleType
Duplicates	Drop identical rows
Normalization	StandardScaler (zero mean, unit variance)
Feature Engineering	e.g., total_amount = price Ã— quantity

ğŸ Output Example
price	quantity	total_amount	scaled_features
100.0	3	300.0	[0.12, -0.43, ...]
